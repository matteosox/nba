# matteosox/nba

## Info

### NBA Stats and Analysis

- Author: Matt Fay
- Email: matt.e.fay@gmail.com
- [Repo](https://github.com/matteosox/nba)
- [Site](https://nba.mattefay.com)

### Description

This repo has three main parts:
1) `pynba`: a Python package of stuff to analyze nba data.
2) `notebooks`: a collection of Jupyter notebooks analyzing nba data using `pynba`.
3) `app`: a Next.js web app hosted at [nba.mattefay.com](https://nba.mattefay.com), displaying the latest stats.

## User Notes

### Jupyter Notebook Environment

_TL;DR: To start up the notebook environment, run `./notebooks/run.sh`, which will open up a browser tab for you._

We use a Dockerized Jupyter notebook environment for data analysis. The `./notebooks/run.sh` bash script starts this container and opens up a web browser to the Jupyter server for you, with the repo mounted to `/home/jupyter/nba`. This allows you to edit the `pynba` package without needing to restart the container, since it is installed in [editable mode](https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs). The Jupyter notebook directory is the repo's `notebooks` directory, which contains version controller notebooks, along with a `data` directory ignored by Git.

## TODO

- Analysis
    - Travel and rest adjustments
    - Automate updates
    - Re-evaluate priors
    - Confirm reduction in home court advantage
    - Fix 2020 bubble games
    - Playoffs?!
- App
    - Theme/style
    - Replace images with interactives
    - Improve tables (sortable, hover for definition, colorize for z-scores)
    - [Incremental static regeneration](https://nextjs.org/docs/basic-features/data-fetching#incremental-static-regeneration)

## Developer Notes

### Getting started

Run `developer_setup.sh`. Right now, all this does is setup the `pre-commit` git hook to build and test code before you commit it.

We use Docker for a clean environment within which to build/test/release. The `build.sh` script in the `cicd` directory will build the relevant images for you. Running the CI/CD workflow natively isn't a supported/maintained thing.

### Documenting changes

_TL;DR: Run `./docs/changelog.sh (added|changed|deprecated|removed|fixed|security) "<message>"` before committing your changes to document them._

We use [`changelog-cli`](https://github.com/mc706/changelog-cli) to document changes from version to version in the `CHANGELOG.md` file. Before committing changes that impact users of the `pynba` package, use the command-line tool to document features added, changed, deprecated, removed, fixed, or security-related changes.

### Versioning

_TL;DR: Run `./docs/release.sh` to summarize unreleased changes in `CHANGELOG.md` and update the package's version._

We version this package, using the syntax defined in [PEP440](https://www.python.org/dev/peps/pep-0440/). For best practices, you can read about it [here](https://the-hitchhikers-guide-to-packaging.readthedocs.io/en/latest/specification.html#sequence-based-scheme).

To simplify this, we use [`changelog-cli`](https://github.com/mc706/changelog-cli) to generate versions for us. This can be done using the `./docs/release.sh` script.

### Code Style

We use PEP8 for Python, but don't trip, just run `./test/black_lint.sh` to get all your spaces in a row.

### Committing Code

We use the `pre-commit` git hook to run the buildin' and testin' phases of our CI/CD pipeline locally.

### Pull Requests

The `main` branch has [branch protections](https://help.github.com/en/github/administering-a-repository/about-protected-branches) turned on in Github, requiring one reviewer to approve a PR before merging. We also use the code owners feature to specify who can approve certain PRs. As well, merging a PR requires status checks to complete successfully.

When naming a branch, please use the syntax `firstname/branch-name-here`. If you plan to collaborate with others on that branch, use `team/branch-name-here`.

### Updating python requirements

_TL;DR: Run `./requirements/update_requirements_in_docker.sh` after building, i.e. `./cicd/build.sh`._

There are two requirements files checked into this directory:
1) `requirements.in`
2) `requirements.txt`

The `.in` files are where we collect immediate dependencies, described in PyPI format (with versions pinned only as needed). The `.txt` files are generated by running the `./requirements/update_requirements_in_docker.sh` script. This script runs the `./requirements/update_requirements.sh` inside the `notebook` Docker container. We do this because `pip-compile` should be run from the same virtual environment as your project so conditional dependencies that require a specific Python version, or other environment markers, resolve relative to your projectâ€™s environment.

This gives us both a flexible way to describe dependencies while still achieving reproducible builds. Inspired by [this](https://hynek.me/articles/python-app-deps-2018/) and [this](https://pythonspeed.com/articles/pipenv-docker/).

### DNS

I own the domain mattefay.com through hover.com. I host my blog there, using format.com. This repo's site is hosted at the nba.mattefay.com subdomain. Since [Vercel](https://vercel.com/) is hosting this site, I have a CNAME DNS record in Hover to alias that subdomain to them, i.e. `CNAME nba cname.vercel-dns.com`.

### Developing the NextJS App

_TL;DR: Run `./app/run.sh` after building, i.e. `./cicd/build.sh`._

To ease developing the NextJS web app, we use `npm run dev` in a Docker container with the app mounted. This starts the app in [development mode](https://nextjs.org/docs/api-reference/cli#development), which takes advantage of NextJS's [fast refresh](https://nextjs.org/docs/basic-features/fast-refresh) functionality, which catches exceptions and loads code updates near-instantaneously.

Additionally, if you'd like to run a different command, e.g. to update the npm packages installed using `npm install`, you can use the same `./app/run.sh` script with a `-c "YOUR CMD HERE"` option.

## Continuous Integration / Continuous Deployment

We use Github actions to run our CI/CD pipeline on every pull request, but every step of CI/CD can also be run locally. 

### Buildin'

_TL;DR: To run tests, run `./cicd/build.sh`._

This builds the two relevant docker images, `notebook`, and `app`.

We do a couple of neat cacheing tricks to speed things up. First off, in the `Dockerfile`s themselves, we use the `RUN --mount=type=cache` functionality of Docker BuildKit to cache Python packages stored in `~/.cache/pip`. This keeps you local machine from re-downloading new Python packages each time. We don't use this for OS-level packages, i.e. those installed using `apt`, to reduce the size of the images. I tried and failed to get this to work for `npm install` and the `node_modules` directory, with mysteriously useless results. This was inspired by [this blog post](https://pythonspeed.com/articles/docker-cache-pip-downloads/)

Second, we use the new `BUILDKIT_INLINE_CACHE` feature to cache our images using Docker Hub. This is configured in the `docker build` command, and is smart enough to only download the layers you need. This was inspired by [this blog post](https://pythonspeed.com/articles/speeding-up-docker-ci/). This DOES work in Github Actions, while the prior functionality does not.

In Github Actions, we use the `--push` flag of the build script to push the images to Docker Hub. Note that you'll need to be logged in to be able to do that locally. We use the `docker/login-action@v1` build action to login, and it uses a personal access token named `github-actions` from my Docker Hub account to do that, with the username and token stored as secrets.

### Testin'

_TL;DR: To run tests, run `./cicd/test.sh`._

#### Python Linting

We both [`Black`](https://black.readthedocs.io/en/stable/index.html) for formatting, and [`pylint`](https://www.pylint.org/) for more general linting. To format your code using Black, simply run `./test/black_lint.sh`.

#### Python Unit Tests

We use the built-in Python module `unittest`'s [test discovery](https://docs.python.org/3/library/unittest.html#test-discovery) functionality. This requires that all of the test files must be modules or packages importable from the root of the repo. As well, they must match the pattern `test*.py`. Our practice is to put tests for a module in a test folder in the same directory, which can then also contain data and other files needed to run those tests.

The package is installed using `setuptools`'s [`find_packages` function](https://setuptools.readthedocs.io/en/latest/setuptools.html#using-find-packages). We use the `exclude` feature to exclude all test code, i.e. `exclude=["*.tests", "*.tests.*", "tests.*", "tests"]`.

Thus, to run tests, we mount the root of the repo to the location in the container it's been installed. All of this is handled nicely by running `test.sh`, which uses the `notebook` container.

#### Web App Tests

**TBD**

### Depolyin'

The Python package `pynba` is strictly for code refactoring in this repo's Jupyter notebook environment, so it isn't packaged up and released to PyPI.org. The NextJS app is deployed to nba.mattefay.com by [Vercel](https://vercel.com/), the company behind NextJS. The deployment process is integrated with Github, so that any commit to the `main` branch results in a new deploy. Conveniently, Vercel also builds and deploys a "staging" site for every commit that changes the `app` directory, making them available through comments in your pull request for example.
