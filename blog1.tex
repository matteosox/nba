\documentclass[12 pt]{article}
\usepackage{amsmath,amstext,amsgen,amsbsy,amsopn,amsfonts,amssymb,graphicx,overcite,theorem}
\begin{document}

\begin{flushleft}

Iâ€™ll start off by describing the simplest model to achieve this.
For each game, we observe the number of points, possessions, home court, and the teams involved.
Then, for each side of the court, we model the points scored (normalized by possessions) as a normal distribution with mean determined by the teams along with home court.

\begin{align}
x_i &\thicksim \mathcal{N}\left(\frac{\text{poss}_i}{100} \left( \mu + \text{off}_i - \text{def}_i + \delta_\text{home} \text{home} \right), \frac{\text{poss}_i}{100}\sigma_{\text{game}}^2 \right) \notag \\
\text{where } x_i &\equiv \text{points scored in the }i^\text{th} \text{ game} \notag \\
\text{poss}_i &\equiv \text{possessions in the }i^\text{th} \text{ game} \notag \\
\delta_\text{home} &\equiv \begin{cases} 1 & \text{if home team is on offense} \\ -1 & \text{otherwise} \end{cases} \notag \\
\mu &\equiv \text{league average scoring rate per 100 possessions} \notag
\end{align}

This model has four parameters of interest:

\begin{enumerate}
\item{$ \overrightarrow{\text{off}} $, the points scored per 100 possessions above league average for each team}
\item{$ \overrightarrow{\text{def}} $, the points allowed per 100 possessions above league average for each team}
\item{$ \text{home} $, the points scored per 100 possessions above league average by the home team}
\item{$ \sigma_{\text{game}} $, the variance for the number of points scored per 100 possessions}
\end{enumerate}

These paremeters each have a prior distribution (since we're being Bayesian about it):

\begin{align}
    \overrightarrow{\text{off}} &\thicksim \mathcal{N}(0, \sigma_{\text{off}} ^ 2) \notag \\
    \overrightarrow{\text{def}} &\thicksim \mathcal{N}(0, \sigma_{\text{def}} ^ 2) \notag \\
    \text{home} &\thicksim \text{Gamma}(1.5, 0.5) \notag \\
    \sigma_{\text{game}} &\thicksim \text{Gamma}(5, 2) \notag
\end{align}

Note that each prior has its own set of parameters, aka hyperparameters, and specifically, that the priors for $ \overrightarrow{\text{off}} $ and $ \overrightarrow{\text{def}} $ have parameters with their own priors, aka hyperpriors.
The $ \text{home} $ and $ \sigma_{\text{game}} $ parameters use a Gamma prior.
This distribution is conveniently continuous and positive, making it a common one for the prior of a positive parameter like the standard deviation.
For that reason, I've also used it for the two hyperpriors:

\begin{align}
    \sigma_{\text{off}} &\thicksim \text{Gamma}(2, 0.5) \notag \\
    \sigma_{\text{def}} &\thicksim \text{Gamma}(1.5, 0.4) \notag \\
\end{align}

I haven't been too careful with setting these priors since this model is mostly for demonstration, but I've run it on the 2019 NBA Regular Season:

[Insert Figure here]

In addition to plotting the offensive and defensive ratings of each team, I've annotated the difference between each team's "raw" ratings and their modeled ones.
We can see that although the model is factoring in strength of schedule and home-court advantage, it's primary effect is "shrinking" the raw ratings.
This is due to the priors for each team rating, which make extreme values less likely.

-

This simple Bayesian Hierarchical Model achieves all our goals, but it left me wanting a bit more.
Specifically, it ignores an immense amount of data, only looking at points per possession.
What if, instead of treating points per possession as a continuous variable, we estimated the probability of a team scoring 0, 1, 2, 3, or 4 points for each possession?
This would both give us useful insights into the way teams succeeded or failed, and also potentially model some of the non-linear dynamics of team matchups.

\vspace{5mm}

Starting off, our model of each game will be a multinomial distribution: given $ n $ trials of an event with probabilities $ p_1, p_2, \cdots, p_m $ for each potential outcome, it models the count of each outcome.
In our case, $ n $ is the number of possessions and the $ p_i $'s are the probabilities of scoring $ i $ points:

\begin{align}
\overrightarrow{x_i} &= [x_{i,0}, x_{i,1}, \cdots, x_{i,4}] \thicksim \text{Multi} \left( \text{poss}_i, \vec{p}_i \right) \notag \\
% \text{where } \overrightarrow{x_i} &= [x_{i,0}, x_{i,1}, \cdots, x_{i,4}] \notag \\
\text{where } \overrightarrow{x_i} &\equiv \text{number of possessions with 0, 1, ..., 4 points scored in the }i^\text{th} \text{ game} \notag \\
\text{poss}_i &\equiv \text{possessions in the }i^\text{th} \text{ game} \notag \\
\vec{p}_i &= [p_{i,0}, p_{i, 1}, \cdots, p_{i,4}] \notag \\
&\equiv \text{probability of 0, 1, ..., 4 points scored in a possession} \notag
\end{align}

Where things get a bit complex is in calculating $ \vec{p}_i $ for each matchup.
Say we know the average NBA team scores zero points in a possession 33\% of the time.
What percentage of possessions do we expect to have zero points when an offense that averages 50\% plays a defense that averages 25\%?
One way to combine these is through odds ratios.

\vspace{5mm}

First off, odds express the relative probability of an event occuring, i.e. adds are the likelihood of an event occurring, divided by the likelihood of it not occurring: $ \text{"odds ratio"} = \frac{p}{1 - p} $.
In our NBA example, the odds of the league average team scoring no points are $ \frac{0.33}{1 - 0.33} \approx 0.5 $, while the offense has odds of $ \frac{0.5}{1 - 0.5} = 1 $, and the defense is at $ \frac{0.25}{1 - 0.25} \approx 0.33 $.
An odds ratio is simply the ratio of the odds for an event, with or without another event. 
A simple, well-behaved way to combine odds ratios is by multiplying them together to get a combined set of odds.

\begin{align}
\text{Given } \mathbb{P}(y) &= p_0, \text{ } \mathbb{P}(y|x_i) = p_i \notag \\
\mathbb{P}(y|x_i, x_2, \cdots, x_n) &= p \notag \\
\text{odds}_i &= \frac{p_i}{1 - p_i} \notag \\
\text{odds} &= \text{odds}_0 \prod_{i=1}^{n} \frac{\text{odds}_i}{\text{odds}_0} \notag
\end{align}

Note that we're actually multiplying the odds ratios, normalized to the league average, which is our prior.
Here, each $x_i$ represents a factor, e.g. that the Phoenix Suns are on offense, that the Philadelphia 76'ers are on defense, or that the home team is on offense.
To get a combined probability out the other end, we simply reverse our original transformation of probability to odds, as follows:

\begin{align}
p &= \frac{\text{odds}}{1 + \text{odds}} \notag
\end{align}

Now that we have a way of combining probabilities to predict observations in a model, we need to define the parameters of our model.
\begin{align}
\overrightarrow{\text{off}}_i &\equiv \text{probability the ith team will score 0, 1, ..., 4 points on offense} \notag \\
\overrightarrow{\text{def}}_i &\equiv \text{probability the ith team will score 0, 1, ..., 4 points on defense} \notag \\
\overrightarrow{\text{home}}_i &\equiv \text{probability the home team will score 0, 1, ..., 4 points on offense} \notag \\
&\text{where } \overrightarrow{\text{off}}_i \text{, } \overrightarrow{\text{def}}_i \text{, and } \overrightarrow{\text{home}} \text{ are of shape } [p_0, p_1, \cdots, p_4] \notag
\end{align}

Defining priors for these model parameters requires introducing a new distribution, the Dirichlet, or multivariate beta distribution.
This distribution is commonly used in Bayesian statistics because it is the conjugate prior of the categorical, and more relevant to us, multinomial distributions.
It is a family of continuous, multivariate probability distributions parameterized by a concentration vector $\overrightarrow{\alpha}$.
We'll use it to define the $ \overrightarrow{\text{off}}_i $, $ \overrightarrow{\text{def}}_i $, and $ \overrightarrow{\text{home}} $ parameters:
\begin{align}
\overrightarrow{\text{off}}_i &\thicksim \text{Dir}(\overrightarrow{\alpha}_{\text{off}}) \notag \\
\overrightarrow{\text{def}}_i &\thicksim \text{Dir}(\overrightarrow{\alpha}_{\text{def}}) \notag \\
\overrightarrow{\text{home}} &\thicksim \text{Dir}(\overrightarrow{\alpha}_{\text{home}}) \notag
\end{align}


The mean of the Dirichlet distribution is defined by an L1-normalized $ \overrightarrow{\alpha} $:
\begin{align}
\operatorname{\mathbb{E}} \left(\text{Dir}(\overrightarrow{\alpha})\right) &= \frac{\overrightarrow{\alpha}}{\alpha_0} \notag \\
\text{where } \alpha_0 &\equiv \sum_i{\alpha_i} \notag
\end{align}

Since the variance is conveniently proportional to $ \frac{1}{\alpha_0^2} $, this allows us to parametrize $ \overrightarrow{\alpha}_{\text{off}} $, $ \overrightarrow{\alpha}_{\text{def}} $, and $ \overrightarrow{\alpha}_{\text{home}} $ as simply proportional to our prior for their probabilities.
For $ \overrightarrow{\text{off}}_i $ and $ \overrightarrow{\text{def}}_i $, our prior is simply league average, but for $ \overrightarrow{\text{home}} $, I used a prior relative to league average based on the improved performance of home teams in previous seasons.
I also chose to fix $ \alpha_{0,\text{home}} $ based on some experimentation, but used hyperpriors for the $ \alpha_{0,\text{off}}$ and $\alpha_{0,\text{def}}$ hyperparameters, using the familiar Gamma distribution:
\begin{align}
\alpha_{0,\text{off}} &\thicksim \text{Gamma}(1{,}000, 300) \notag \\
\alpha_{0,\text{def}} &\thicksim \text{Gamma}(2{,}000, 500) \notag \\
\alpha_{0,\text{home}} &\equiv 100{,}000 \notag
\end{align}

Running this new model on the 2019 NBA Regular Season yields similar results despite a completely different model formulation:

[Insert Figure here]

What I prefer about this model is it provides more explanation for how each team achieves its results on each side of the floor, looking at the results of a possession on a more granular level by modeling individual outcomes instead of the average one.
Unfortunately, this model doesn't really speak the "language of basketball"; no one refers to a team's probability of scoring 0 points on a possession for example.
Instead, basketball analysts focus on the process of scoring with statistics like shooting percentage, rebounds, and turnovers.
What we'd really like to model is the relationship between these process metrics, and scoring outcomes.

\vspace{5mm}

To do that, I've modified Dean Oliver's "Four Factors of Basketball Success," replacing them with seven:
\begin{enumerate}
\item{$ \frac{\text{3pt att}}{\text{shot}} $: percentage of shots that were attempted from three point range}
\item{$ \text{2pt}\% $: percentage of two point shot attempts made}
\item{$ \text{3pt}\% $: percentage of three point shot attempts made}
\item{$ \text{reb}\% $: percentage of rebounds grabbed}
\item{$ \text{to}\% $: percentage of possessions that end in a turnover}
\item{$ \frac{\text{ft att}}{\text{poss}} $: free throws attempted per possession}
\item{$ \text{ft}\% $: percentage of free throw attempts made}
\end{enumerate}

The math is a bit complicated, but with these seven factors, we can accurately estimate the points scored per possession.

\begin{align}
\frac{\text{points}}{\text{poss}} &= \frac{\text{shots}}{\text{poss}} \times \frac{\text{points}}{\text{shot}} + \frac{\text{ft att}}{\text{poss}} \times \text{ft}\% \notag \\
\text{where } \frac{\text{points}}{\text{shot}} &= 3 \times \frac{\text{3pt att}}{\text{shot}} \times \text{3pt}\% + 2 \times \left(1 - \frac{\text{3pt att}}{\text{shot}} \right) \times \text{2pt}\% \notag
\end{align}

These equations have one undefined term which must be estimated: $ \frac{\text{shots}}{\text{poss}} $.
A possession can have any number of shots thanks to the resetting nature of a missed shot attempt combined with an offensive rebound.
To clarify this, we'll need a new term: a shot opportunity. Every possession starts with a shot opportunity, and has additional shot opportunities for each offensive rebound, which acts to reset the possession.

\begin{align}
\frac{\text{shots}}{\text{poss}} &= 0 \times \mathbb{P}(0) + 1 \times \mathbb{P}(1) + 2 \times \mathbb{P}(2) + \cdots \notag \\
\text{where } \mathbb{P}(k) &\cong p_\text{last shot} \times p_\text{reset}^{k - 1} \notag \\
p_\text{last shot} &\equiv p_\text{shot} \times \left( p_\text{make} + p_\text{miss} \times \left(p_\text{def reb} + p_\text{off reb} \times \left(1 - p_\text{shot} \right) \right) \right) \notag \\
p_\text{reset} &\equiv p_\text{shot} \times p_\text{miss} \times p_\text{off reb} \notag \\
p_\text{make} &\equiv \frac{\text{3pt att}}{\text{shot}} \times \text{3pt}\% + \left(1 - \frac{\text{3pt att}}{\text{shot}} \right) \times \text{2pt}\% \notag \\
p_\text{miss} &\equiv 1 - p_\text{make} \notag
\end{align}

Note that I've labeled that $ \mathbb{P}(k) $ here is approximate. We're assuming that the events in a possession are independent, i.e. when a possession resets with a misses shot \& offensive rebound, this is the same state as the beginning of a possession.
Of course, this isn't true, since an offensive rebound can often lead to an immediate "putback" attempt, the shot clock is usually set to 14 instead of the usual 24, there aren't fast breaks, and so on. Still, empirically, this assumption seems to work.

\vspace{5mm}

That set aside, we're again left with one undefined: $ p_\text{shot} $, the probability of attempting a shot given a shot opportunity.
Turnovers prevent shots, but less obviously, so do shooting fouls, since they don't count as a shot attempt if the shot is missed.
Thus, to estimate this factor, I ran a simple linear regression to estimate it:

\begin{align}
p_\text{shot} &\cong k_\text{1} \times 1 + k_\text{to} \times \text{to}\% + k_\text{fta} \times \frac{\text{ft att}}{\text{poss}} \notag \\
&= 1 - 0.85 \times \text{to}\% - 0.4 \times \frac{\text{ft att}}{\text{poss}} \notag
\end{align}

With all these terms defined, we can reformulate $ \frac{\text{shots}}{\text{poss}} $ as the sum of an infinite series:

\begin{align}
\frac{\text{shots}}{\text{poss}} &= \sum_{k=0}^{\infty}{ k \times p_\text{last shot} \times p_\text{reset}^{k - 1}} \notag \\
&= p_\text{last shot} \sum_{k=1}^{\infty}{ k \times p_\text{reset}^{k - 1}} \notag \\
&= \frac{p_\text{last shot}}{p_\text{reset}} \sum_{k=1}^{\infty}{ k \times p_\text{reset}^k} \notag \\
&= \frac{p_\text{last shot}}{p_\text{reset}} \times \frac{p_\text{reset}}{(1 - p_\text{reset})^2} \notag \\
&= \frac{p_\text{last shot}}{(1 - p_\text{reset})^2} \notag
\end{align}

All that work done, we're left with an almost exact method of calculating a team's points per possession given the seven factors.
So how did I model those seven factors?
Well conveniently, they can be each modeled independently of one another.
As well, six of the seven are simple binary probabilities, allowing me to fit the data using a simple Binomial distribution and Beta priors, much easier than the Multinomial distribution and Dirichlet priors from before.
For the one continuous variable ($ \frac{\text{ft att}}{\text{poss}} $), I used a simple Normal distribution with Gamma priors.
As well, I modeled the pace of each game (seconds per possession) using a similar approach, using a Gamma distribution for both the observations and priors.
Most of these parameters depend on the offense, defense, and home court, with the following exceptions:
\begin{enumerate}
    \item{$ \frac{\text{3pt att}}{\text{shot}} $ didn't depend on home court}
    \item{pace didn't depend on home court}
    \item{$ \text{ft}\% $ didn't depend on the defense}
\end{enumerate}

Running this final model on the 2019 NBA Regular Season, we see similar patterns of shrinkage of extreme values, but with more variation, due to the model factoring in more inputs:

Finally, we also get a nice new plot, showing the pace for each team. Here we see that the model is doing very little in comparison to the raw values:

\end{flushleft}

\end{document}